import os
import sys
import subprocess
import pandas as pd
import re
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import roc_auc_score, average_precision_score, roc_curve

# ===============================
# è·¯å¾„è®¾ç½®
# ===============================
BASE_DIR = "/Users/krt/krt files/Github/new_stat/credit_code"
ROOT_DIR = os.path.dirname(BASE_DIR)  # /Users/krt/krt files/Github/new_stat
OUT = os.path.join(ROOT_DIR, "outputs")  # âœ… è¾“å‡ºæ–‡ä»¶å¤¹ä¸ credit_code åŒçº§
DATA_DIR = os.path.join(ROOT_DIR, "data")
os.makedirs(OUT, exist_ok=True)

# ===============================
# æ¨¡å‹è„šæœ¬åˆ—è¡¨
# ===============================
scripts = [
    "train_logistic.py",
    "train_extratrees.py",
    "train_xgboost.py",
    "train_linear_regression.py"
]

# ===============================
# æ‰§è¡Œæ¨¡å‹è„šæœ¬
# ===============================
def run_script(script):
    print(f"\nğŸš€ æ­£åœ¨è¿è¡Œæ¨¡å‹è„šæœ¬ï¼š{script}")
    result = subprocess.run([sys.executable, os.path.join(BASE_DIR, script)],
                            capture_output=True, text=True)
    print(result.stdout)
    if result.stderr.strip():
        print("âš ï¸ é”™è¯¯ä¿¡æ¯ï¼š\n", result.stderr)
    return result.stdout

# ===============================
# ä»æ—¥å¿—ä¸­æå–æŒ‡æ ‡
# ===============================
def parse_metrics(log_text):
    metrics = {"AUC": np.nan, "AP": np.nan, "KS": np.nan}
    match = re.search(r"AUC=([\d\.]+).*AP=([\d\.]+).*KS=([\d\.]+)", log_text)
    if match:
        metrics = {
            "AUC": float(match.group(1)),
            "AP": float(match.group(2)),
            "KS": float(match.group(3))
        }
    return metrics

# ===============================
# å›¾è¡¨æ˜¾ç¤ºä¸­æ–‡
# ===============================
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

# ===============================
# è¿è¡Œå„æ¨¡å‹å¹¶æ”¶é›†æŒ‡æ ‡
# ===============================
results = {}
for script in scripts:
    log_output = run_script(script)
    metrics = parse_metrics(log_output)
    results[script.replace(".py", "")] = metrics

# ===============================
# è¯»å–é¢„æµ‹æ–‡ä»¶
# ===============================
print("\nâš™ï¸ æ­£åœ¨åŠ è½½æ¨¡å‹é¢„æµ‹æ–‡ä»¶...")
files = {
    "logistic": os.path.join(OUT, "submission_logistic_regression.csv"),
    "extratrees": os.path.join(OUT, "submission_extra_trees.csv"),
    "xgboost": os.path.join(OUT, "submission_xgboost.csv"),
    "linear": os.path.join(OUT, "submission_linear_regression.csv")
}

dfs = {}
for name, path in files.items():
    if os.path.exists(path):
        try:
            dfs[name] = pd.read_csv(path)
            print(f"âœ… å·²åŠ è½½ï¼š{name} ({dfs[name].shape[0]} è¡Œ)")
        except Exception as e:
            print(f"âš ï¸ è¯»å– {name} æ–‡ä»¶å¤±è´¥ï¼š{e}")
    else:
        print(f"âš ï¸ æœªæ‰¾åˆ° {name} æ–‡ä»¶ï¼š{path}")

if len(dfs) == 0:
    raise RuntimeError("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ¨¡å‹é¢„æµ‹ç»“æœï¼Œæ— æ³•æ‰§è¡Œèåˆï¼")

# ===============================
# è‡ªåŠ¨é€‰æ‹© AUC æœ€é«˜çš„ä¸¤ä¸ªæ¨¡å‹
# ===============================
df_compare = pd.DataFrame(results).T
best_models = df_compare["AUC"].sort_values(ascending=False).head(2).index.tolist()
best_names = [b.replace("train_", "") for b in best_models]
print(f"\nğŸ† è‡ªåŠ¨é€‰æ‹©è¡¨ç°æœ€å¥½çš„ä¸¤ä¸ªæ¨¡å‹ç”¨äºèåˆï¼š{best_names}")

# æå–å¯¹åº”çš„é¢„æµ‹ç»“æœ
available_models = {k:v for k,v in dfs.items() if any(k in b for b in best_names)}
if len(available_models) < 2:
    print("âš ï¸ æ‰¾ä¸åˆ°è¶³å¤Ÿçš„æ¨¡å‹ç”¨äºèåˆï¼Œé»˜è®¤èåˆå…¨éƒ¨å¯ç”¨æ¨¡å‹ã€‚")
    available_models = dfs

# ===============================
# è‡ªåŠ¨ç¡®å®š ID å¯¹é½æ¨¡æ¿
# ===============================
main_key = list(available_models.keys())[0]
ids = available_models[main_key]["id"]
for k in available_models:
    available_models[k] = available_models[k].set_index("id").reindex(ids).reset_index()

# ===============================
# åŠ¨æ€è®¡ç®—æƒé‡ï¼ˆæŒ‰AUCå æ¯”ï¼‰
# ===============================
auc_dict = {k: df_compare.loc[f"train_{k}", "AUC"] if f"train_{k}" in df_compare.index else 0 for k in available_models}
sum_auc = sum(auc_dict.values())
weights = {k: (v / sum_auc if sum_auc > 0 else 1/len(available_models)) for k, v in auc_dict.items()}

print("\nğŸ“¦ æ¨¡å‹èåˆæƒé‡ï¼ˆæŒ‰AUCè‡ªé€‚åº”ï¼‰ï¼š")
for k, v in weights.items():
    print(f"  {k:<12} â†’ {v:.2f}")

# ===============================
# è®¡ç®—èåˆæ¦‚ç‡
# ===============================
ensemble_prob = np.zeros(len(ids), dtype=float)
for name, df in available_models.items():
    ensemble_prob += df["target"].values * weights[name]

sub_ensemble = pd.DataFrame({"id": ids, "target": ensemble_prob})
ensemble_path = os.path.join(OUT, "submission_ensemble.csv")
sub_ensemble.to_csv(ensemble_path, index=False)
print(f"\nâœ… å·²ç”Ÿæˆèåˆé¢„æµ‹æ–‡ä»¶ï¼š{ensemble_path}")

# ===============================
# èåˆæ¨¡å‹è¯„ä¼°
# ===============================
train_path = os.path.join(DATA_DIR, "è®­ç»ƒæ•°æ®é›†.xlsx")
if os.path.exists(train_path):
    train = pd.read_excel(train_path, sheet_name=0)
    if "target" in train.columns:
        y_true = train["target"].astype(int).values
        ensemble_valid = ensemble_prob[:len(y_true)]
        auc = roc_auc_score(y_true, ensemble_valid)
        ap = average_precision_score(y_true, ensemble_valid)
        fpr, tpr, _ = roc_curve(y_true, ensemble_valid)
        ks = np.max(np.abs(tpr - fpr))

        plt.figure(figsize=(6, 5))
        plt.plot(fpr, tpr, label=f"AUC={auc:.4f}, KS={ks:.4f}")
        plt.plot([0, 1], [0, 1], "--", color="gray")
        plt.xlabel("å‡é˜³æ€§ç‡ (FPR)")
        plt.ylabel("çœŸé˜³æ€§ç‡ (TPR)")
        plt.title("èåˆæ¨¡å‹ ROC æ›²çº¿ï¼ˆTop-2ï¼‰")
        plt.legend()
        plt.tight_layout()
        plt.savefig(os.path.join(OUT, "ensemble_roc_curve.png"), dpi=200)
        plt.close()

        results["ensemble_submission_top2"] = {"AUC": auc, "AP": ap, "KS": ks}
        print(f"ğŸ“Š èåˆæ¨¡å‹ï¼ˆTop-2ï¼‰è¡¨ç°ï¼šAUC={auc:.4f}, AP={ap:.4f}, KS={ks:.4f}")
    else:
        print("âš ï¸ è®­ç»ƒæ•°æ®ç¼ºå°‘ target åˆ—ï¼Œæ— æ³•è®¡ç®—æŒ‡æ ‡ã€‚")
else:
    print("âš ï¸ æœªæ‰¾åˆ°è®­ç»ƒæ•°æ®é›†æ–‡ä»¶ï¼Œè·³è¿‡è¯„ä¼°ã€‚")

# ===============================
# æ±‡æ€»æŒ‡æ ‡ä¸ç»˜å›¾
# ===============================
df_compare = pd.DataFrame(results).T
df_compare.to_csv(os.path.join(OUT, "model_comparison.csv"), index_label="æ¨¡å‹åç§°")
print("\nâœ… æ¨¡å‹æ€§èƒ½å¯¹æ¯”è¡¨å·²ä¿å­˜ï¼šmodel_comparison.csv")
print(df_compare.round(4))

# ===============================
# ç»˜åˆ¶æ¨¡å‹æŒ‡æ ‡å¯¹æ¯”å›¾
# ===============================
if not df_compare.empty:
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))
    metrics = ["AUC", "AP", "KS"]
    colors = ["#409EFF", "#67C23A", "#E6A23C"]
    for i, metric in enumerate(metrics):
        ax = axes[i]
        df_compare[metric].plot(kind="bar", ax=ax, color=colors[i])
        ax.set_title(f"{metric} æŒ‡æ ‡å¯¹æ¯”", fontsize=13)
        ax.set_xlabel("æ¨¡å‹")
        ax.set_ylabel(metric)
        ax.grid(axis="y", linestyle="--", alpha=0.7)
        for idx, val in enumerate(df_compare[metric]):
            ax.text(idx, val, f"{val:.3f}", ha="center", va="bottom", fontsize=9)
    plt.suptitle("å„æ¨¡å‹æ€§èƒ½å¯¹æ¯”å›¾ï¼ˆå« Top-2 èåˆï¼‰", fontsize=15)
    plt.tight_layout(rect=[0, 0, 1, 0.95])
    plot_path = os.path.join(OUT, "model_comparison_plot.png")
    plt.savefig(plot_path, dpi=200)
    plt.close()
    print(f"ğŸ“ˆ æ¨¡å‹æ€§èƒ½å¯¹æ¯”å›¾å·²ä¿å­˜ï¼š{plot_path}")
else:
    print("âš ï¸ æ— æ¨¡å‹æŒ‡æ ‡æ•°æ®ï¼Œæ— æ³•ç»˜åˆ¶å¯¹æ¯”å›¾ã€‚")

print("\nğŸ¯ å…¨éƒ¨æµç¨‹æ‰§è¡Œå®Œæ¯•ï¼è¾“å‡ºæ–‡ä»¶å‡ä½äºï¼š", OUT)
