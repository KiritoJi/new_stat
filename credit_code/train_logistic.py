# -*- coding: utf-8 -*-
import os, numpy as np, pandas as pd, matplotlib.pyplot as plt
from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score, average_precision_score, roc_curve
from sklearn.base import clone

# ===============================
# è·¯å¾„è®¾ç½®
# ===============================
BASE_DIR = "/Users/krt/krt files/Github/new_stat/credit_code"
ROOT_DIR = os.path.dirname(BASE_DIR)
DATA_DIR = os.path.join(ROOT_DIR, "data")
OUT = os.path.join(ROOT_DIR, "outputs")
os.makedirs(OUT, exist_ok=True)

TRAIN = os.path.join(DATA_DIR, "è®­ç»ƒæ•°æ®é›†.xlsx")
TEST = os.path.join(DATA_DIR, "æµ‹è¯•é›†.xlsx")
SAMPLE = os.path.join(DATA_DIR, "æäº¤æ ·ä¾‹.csv")

# ===============================
# å·¥å…·å‡½æ•°ï¼šåˆ—åç»Ÿä¸€ & é«˜é£é™©purposeå­¦ä¹ 
# ===============================
def normalize_columns(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    df.columns = [str(c).strip().lower() for c in df.columns]
    return df

def learn_high_risk_purpose(train_df: pd.DataFrame, target_col: str):
    if "purpose" not in train_df.columns:
        return set()
    grp = train_df.groupby("purpose")[target_col].mean()
    global_mean = train_df[target_col].mean()
    global_std = train_df[target_col].std(ddof=0)
    # é˜ˆå€¼ = å…¨å±€å‡å€¼ + 1Ïƒï¼›è‹¥Ïƒæå°ï¼Œåˆ™å– TopN(3)
    if pd.isna(global_std) or global_std < 1e-6:
        high = set(grp.sort_values(ascending=False).head(3).index)
    else:
        high = set(grp[grp >= global_mean + global_std].index)
        if len(high) == 0:  # ä¿åº•è‡³å°‘2ä¸ª
            high = set(grp.sort_values(ascending=False).head(2).index)
    return high

# ===============================
# ç‰¹å¾å·¥ç¨‹ï¼ˆæ–°æ–¹æ³•ï¼šæ¯”å€¼/å€’æ•°/äº¤äº’/æ ‡è¯†ï¼‰
# ===============================
def add_business_features(df: pd.DataFrame, high_risk_purposes=set()) -> pd.DataFrame:
    df = df.copy()
    # ç»Ÿä¸€ç±»å‹
    for c in ["purpose", "housing"]:
        if c in df.columns:
            df[c] = df[c].astype(str).str.lower()

    # æ¯”å€¼ç±»
    if "amount" in df.columns and "income" in df.columns:
        inc = df["income"].replace(0, np.nan).astype(float)
        df["loan_income_ratio"] = (df["amount"].astype(float) / inc).fillna(0).clip(0, 100)

    if "credict_used_amount" in df.columns and "credict_limit" in df.columns:
        lim = df["credict_limit"].replace(0, np.nan).astype(float)
        df["credit_utilization_ratio"] = (df["credict_used_amount"].astype(float) / lim).fillna(0).clip(0, 10)

    if "amount" in df.columns and "credict_limit" in df.columns:
        lim = df["credict_limit"].replace(0, np.nan).astype(float)
        df["amount_to_limit"] = (df["amount"].astype(float) / lim).fillna(0).clip(0, 100)

    # å€’æ•°ç±»ï¼ˆè¿‘æœŸæ•ˆåº”ï¼‰
    for c in ["last_overdue_months", "last_credict_card_months"]:
        if c in df.columns:
            s = df[c].astype(float)
            df[f"inv_{c}"] = (1.0 / (1.0 + s.fillna(s.median()))).clip(0, 1)

    # å†å²ä¸è‰¯ç»¼åˆ
    if "overdue_times" in df.columns and "default_times" in df.columns:
        df["risk_exposure_index"] = df["overdue_times"].fillna(0).astype(float) + \
                                    2.0 * df["default_times"].fillna(0).astype(float)

    # housing æŒ‡ç¤º
    if "housing" in df.columns:
        df["is_rent"] = df["housing"].eq("rent").astype(int)
        df["is_own"] = df["housing"].eq("own").astype(int)

    # purpose é«˜é£é™©æ ‡è¯†ï¼ˆæ•°æ®é©±åŠ¨ï¼‰
    if "purpose" in df.columns:
        df["purpose_risk_flag"] = df["purpose"].isin(high_risk_purposes).astype(int)

    # äº¤äº’é¡¹ï¼ˆçº¿æ€§æ¨¡å‹å—ç›Šæ˜æ˜¾ï¼‰
    if "loan_income_ratio" in df.columns and "is_rent" in df.columns:
        df["int_ratio_rent"] = df["loan_income_ratio"] * df["is_rent"]
    if "credit_utilization_ratio" in df.columns and "inv_last_overdue_months" in df.columns:
        df["int_util_recentoverdue"] = df["credit_utilization_ratio"] * df["inv_last_overdue_months"]

    # log æ”¶å…¥ï¼ˆæ§åˆ¶ååº¦ï¼‰
    if "income" in df.columns:
        df["log_income"] = np.log1p(df["income"].astype(float))

    return df

def ks_score(y_true, y_prob):
    fpr, tpr, _ = roc_curve(y_true, y_prob)
    return float(np.max(np.abs(tpr - fpr)))

# ===============================
# æ•°æ®è¯»å– & ç‰¹å¾å­¦ä¹ 
# ===============================
train_raw = normalize_columns(pd.read_excel(TRAIN, sheet_name=0))
test_raw = normalize_columns(pd.read_excel(TEST, sheet_name=0))
target_col = "target"

high_risk_purposes = learn_high_risk_purpose(train_raw, target_col)
train = add_business_features(train_raw, high_risk_purposes)
test = add_business_features(test_raw, high_risk_purposes)
sample = pd.read_csv(SAMPLE)

# æ‹†åˆ—
cat_cols = [c for c in train.columns if train[c].dtype == "object" and c != target_col]
num_cols = [c for c in train.columns if c not in cat_cols + [target_col]]
if "id" in num_cols: num_cols.remove("id")

X = train[cat_cols + num_cols].reset_index(drop=True)
y = train[target_col].astype(int).reset_index(drop=True)

# ===============================
# é¢„å¤„ç† & æ¨¡å‹
# ===============================
num_tf = Pipeline([
    ("imp", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler())
])
cat_tf = Pipeline([
    ("imp", SimpleImputer(strategy="most_frequent")),
    ("ohe", OneHotEncoder(handle_unknown="ignore"))
])
pre = ColumnTransformer([
    ("num", num_tf, num_cols),
    ("cat", cat_tf, cat_cols)
])

pipeline = Pipeline([
    ("pre", pre),
    ("clf", LogisticRegression(
        solver="liblinear",
        class_weight="balanced",
        max_iter=600,
        C=1.0
    ))
])

# ===============================
# 10 æ¬¡éšæœºåˆ’åˆ† + å‚æ•°æœç´¢
# ===============================
cv_inner = StratifiedShuffleSplit(n_splits=10, test_size=0.25, random_state=42)
param_grid = {
    "clf__C": [0.1, 0.3, 1.0, 3.0, 10.0],
    "clf__penalty": ["l1", "l2"]
}

print("ğŸ” Logistic å›å½’ï¼šå¼€å§‹ 10 æ¬¡éšæœºåˆ’åˆ† + ç½‘æ ¼æœç´¢è°ƒå‚ï¼ˆæŒ‡æ ‡ï¼šAUCï¼‰...")
grid = GridSearchCV(
    pipeline,
    param_grid=param_grid,
    scoring="roc_auc",
    cv=cv_inner,
    n_jobs=-1,
    refit=True,
    verbose=0
)
grid.fit(X, y)
best_model = grid.best_estimator_
print(f"âœ… æœ€ä¼˜å‚æ•°ï¼š{grid.best_params_}ï¼Œå¹³å‡AUC={grid.best_score_:.4f}")

# ä½¿ç”¨æ–°çš„éšæœºåˆ’åˆ†è¯„ä¼°æœ€ä½³æ¨¡å‹çš„ç¨³å®šæ€§ï¼ˆè¾“å‡º AUC/AP/KS å‡å€¼ï¼‰
cv_eval = StratifiedShuffleSplit(n_splits=10, test_size=0.25, random_state=123)
auc_scores, ap_scores, ks_scores = [], [], []
for fold_idx, (tr_idx, val_idx) in enumerate(cv_eval.split(X, y), 1):
    model = clone(best_model)
    model.fit(X.iloc[tr_idx], y.iloc[tr_idx])
    y_prob = model.predict_proba(X.iloc[val_idx])[:, 1]
    y_val = y.iloc[val_idx]
    auc_scores.append(roc_auc_score(y_val, y_prob))
    ap_scores.append(average_precision_score(y_val, y_prob))
    ks_scores.append(ks_score(y_val, y_prob))

auc = float(np.mean(auc_scores))
ap = float(np.mean(ap_scores))
ks = float(np.mean(ks_scores))
print(f"[Logistic Regression] CV AUC={auc:.4f}  AP={ap:.4f}  KS={ks:.4f}")

# æœ€ç»ˆæ¨¡å‹ï¼šåœ¨å…¨éƒ¨è®­ç»ƒæ•°æ®ä¸Šé‡æ‹Ÿåˆ
best_model.fit(X, y)

# é¢„æµ‹ & å¯¼å‡ºï¼ˆåŸºäºé‡è®­ç»ƒåçš„æ¨¡å‹ï¼‰
prob_test = best_model.predict_proba(test[cat_cols + num_cols])[:, 1]
prob_test = np.where(prob_test < 0.1, prob_test / 10.0, prob_test)
sample["target"] = prob_test
out_path = os.path.join(OUT, "submission_logistic_regression.csv")
sample.to_csv(out_path, index=False)
print("âœ… æ¦‚ç‡é¢„æµ‹æ–‡ä»¶ï¼š", out_path)

# ç‰¹å¾é‡è¦æ€§ï¼ˆç³»æ•°ç»å¯¹å€¼ Top30ï¼‰ï¼ŒåŸºäºé‡è®­ç»ƒåçš„æ¨¡å‹
ohe = best_model.named_steps["pre"].named_transformers_["cat"].named_steps["ohe"]
cat_feature_names = ohe.get_feature_names_out(cat_cols)
feature_names = np.concatenate([num_cols, cat_feature_names])
coef = best_model.named_steps["clf"].coef_[0]
imp_df = pd.DataFrame({"feature": feature_names, "importance": np.abs(coef)}).sort_values("importance", ascending=False).head(30)

plt.figure(figsize=(9, 7))
plt.barh(imp_df["feature"], imp_df["importance"])
plt.gca().invert_yaxis()
plt.title("Top 30 Feature Importances (Logistic)")
plt.tight_layout()
plt.savefig(os.path.join(OUT, "logistic_feature_importance.png"), dpi=220)
plt.close()
print("ğŸ“Š ç‰¹å¾é‡è¦æ€§å›¾å·²ä¿å­˜ã€‚")
